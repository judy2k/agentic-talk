{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a0f3c3-1bd9-4d5f-960d-4481559a0087",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You'll need to set environment variables for your secret keys: `OPENAI_API_KEY` and `ANTHROPIC_API_KEY`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd8bf24c-fd47-4fa0-9edf-1897f700bb41",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 22ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install llama-index \\\n",
    "    llama-index-embeddings-openai llama-index-llms-anthropic llama-index-llms-ollama  \\\n",
    "    llama-index-vector-stores-MongoDB \\\n",
    "    pymongo \\\n",
    "    vonage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3012efec-f6f1-4539-8c14-f7f34915817a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for some required environment variables:\n",
    "import os\n",
    "\n",
    "# Check all required environment variables have been set:\n",
    "for required_env_var in [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]:\n",
    "    try:\n",
    "        assert os.environ[required_env_var] is not None\n",
    "    except KeyError:\n",
    "        print(f\"You must set {required_env_var} to run this notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c79ba0-f11c-4d02-83bb-8db46af06293",
   "metadata": {},
   "source": [
    "# Create an LLM object.\n",
    "\n",
    "An LLM object provides an interface to send prompts to an LLM.\n",
    "In this case, Anthropic credentials are silently provided by an env-var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "259d13da-4d30-4cb6-9d7d-52fd674864d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "anthropic_llm = Anthropic(model=\"claude-3-5-sonnet-20241022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eaf770-ed25-4bc1-9092-c436c87fdc9b",
   "metadata": {},
   "source": [
    "# Create Some Maths Tools\n",
    "\n",
    "This creates two tools, `multiply_tool` and `add_tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f43a16-524f-40c5-835d-8645109d13e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two numbers.\n",
    "    \n",
    "    Provide three parameters: a and b (the operands).\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers.\n",
    "    \n",
    "    Provide two parameters: a and b (the operands).\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "\n",
    "# What's the difference between a tool and a function?\n",
    "# Muppet reminder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477674db-7de7-496c-b721-1fc0e1956b6d",
   "metadata": {},
   "source": [
    "## Create an Agent\n",
    "\n",
    "Create a ReActAgent with the provided tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1633b-b7ab-49d5-b5f8-284c321a4f14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(tools=[multiply_tool, add_tool], llm=anthropic_llm, verbose=True)\n",
    "response = agent.chat(\"Solve ((5*4)+8)\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ccce4-6d98-486a-8418-b3eed5792c32",
   "metadata": {},
   "source": [
    "# Create a Tool to Send SMS\n",
    "\n",
    "This tool will use the vonage API to send an SMS.\n",
    "It's preconfigured to send messages to either my \"wife\" or my \"manager\".\n",
    "\n",
    "(In fact all SMS messages will be routed to my phone.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66153742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vonage\n",
    "\n",
    "def send_sms(recipient: str, message: str):\n",
    "    \"\"\"\n",
    "    Send an SMS to my manager or wife.\n",
    "    \n",
    "    The recipient argument should be either \"manager\" or \"wife\".\n",
    "    The message argument should contain the message body.\n",
    "    \"\"\"\n",
    "    destinations = {\n",
    "        \"manager\": os.environ['DEST_PHONE_NUMBER'],\n",
    "        \"wife\": os.environ['DEST_PHONE_NUMBER'],\n",
    "    }\n",
    "\n",
    "    nexmo_client = vonage.Client(key=os.environ['NEXMO_API_KEY'], secret=os.environ['NEXMO_API_SECRET'])\n",
    "    nexmo_client.sms.send_message({\n",
    "        \"from\": os.environ['FROM_PHONE_NUMBER'],\n",
    "        \"to\": destinations[recipient],\n",
    "        'text': message,\n",
    "    })\n",
    "\n",
    "send_sms_tool = FunctionTool.from_defaults(send_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbfa7f-8490-4572-b264-fd00698150ae",
   "metadata": {},
   "source": [
    "## Test Sending an SMS\n",
    "\n",
    "Let's recreate that agent, and tell it to send an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c69bb8-ad97-4cc1-a1ec-f521cfa0ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools([multiply_tool, add_tool, send_sms_tool], llm=anthropic_llm, verbose=True)\n",
    "agent.chat(\"Send an SMS to my boss telling him I'm going to be late for work.\")\n",
    "\n",
    "# It's 6:30pm and I'm still at work.\n",
    "# It's 8:45am and I'm in the car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1148dbb-1d2e-400a-a159-15b28017cb32",
   "metadata": {},
   "source": [
    "# Where is the LLM getting its instructions?\n",
    "\n",
    "We've seen that the LLM will take actions without being given instructions. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd17fef-2cb1-4cb0-97f7-73c066224c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, add_tool, send_sms_tool], llm=anthropic_llm, verbose=True)\n",
    "\n",
    "for k, v in agent.get_prompts().items():\n",
    "    print(f\"Prompt: {k}\\n\\n{v.template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659e820",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(os.environ['MONGODB_URI'])\n",
    "\n",
    "class ActionLog:\n",
    "    \"\"\"\n",
    "    Stores actions in MongoDB, and can retrieve all actions taken in the past day.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mongodb_client):\n",
    "        self._client = mongodb_client\n",
    "        self.db = self._client.get_default_database()\n",
    "        self.action_log = self.db.get_collection('action_log')\n",
    "    \n",
    "    def log_action(self, action_description: str):\n",
    "        \"\"\" Record something you have done, so that it can be retrieved later.\n",
    "        \n",
    "        The action_description parameter should be a description of an individual action you have taken.\n",
    "        \"\"\"\n",
    "        self.action_log.insert_one({\n",
    "            'when': datetime.now(),\n",
    "            'description': action_description,\n",
    "        })\n",
    "\n",
    "    def get_recent_actions(self):\n",
    "        return list(self.action_log.find({\n",
    "            'when': {\"$gt\": datetime.now() - timedelta(days=11)}\n",
    "        }, {'_id': 0}).sort({'when': -1}))\n",
    "\n",
    "action_log = ActionLog(mongodb_client)\n",
    "\n",
    "log_action_tool = FunctionTool.from_defaults(action_log.log_action)\n",
    "recent_actions_tool = FunctionTool.from_defaults(action_log.get_recent_actions)\n",
    "\n",
    "action_log.get_recent_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d56540",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, add_tool, send_sms_tool, log_action_tool, recent_actions_tool], llm=anthropic_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\"It is 8:30am. Send an SMS to my boss to tell him I'm running late. Make up a plausible excuse.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345a8df-cead-48ec-87ad-e7d241f3182b",
   "metadata": {},
   "source": [
    "# Creating a store of facts\n",
    "\n",
    "This will be a vector index storing everything I can think of that might be useful to the LLM in making decisions.\n",
    "The LLM can use RAG techniques to look up anything it might need to know, and then can take action based on interpreting the results.\n",
    "\n",
    "## Configure an Embedding Model\n",
    "\n",
    "Vector search requires the data to be turned into an embedding, both when the data is loaded, and when the data is queried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f346409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an embedding model\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\", \n",
    "    dimensions=256,\n",
    "    embed_batch_size=10, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55282969-e711-41e6-9608-9c347ee5fd08",
   "metadata": {},
   "source": [
    "## Create a Vector Index in MongoDB\n",
    "\n",
    "This is slightly more complex than it could be, because vector indexes are created asynchronously, in a different service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d397581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector index on the \"facts\" collection:\n",
    "import time\n",
    "\n",
    "from pymongo.collection import Collection\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "db = mongodb_client.get_default_database()\n",
    "\n",
    "# If there isn't a \"facts\" collection, then create one:\n",
    "if \"facts\" not in db.list_collection_names(filter={\"name\": \"facts\"}):   \n",
    "    db.create_collection(\"facts\")\n",
    "\n",
    "# If there isn't a \"facts_index\" vector index, then create one:\n",
    "facts_collection: Collection = db.get_collection(\"facts\")\n",
    "if not list(facts_collection.list_search_indexes(name=\"facts_index\")):\n",
    "    print(\"Creating vector index ...\")\n",
    "    facts_collection.create_search_index(model=SearchIndexModel({\n",
    "        \"fields\": [\n",
    "            {\n",
    "            \"numDimensions\": 256,\n",
    "            \"path\": \"embedding\",\n",
    "            \"similarity\": \"cosine\",\n",
    "            \"type\": \"vector\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        name=\"facts_index\",\n",
    "        type=\"vectorSearch\"))\n",
    "    \n",
    "    # Wait for it to be created:\n",
    "    while True:\n",
    "        indexes = list(facts_collection.list_search_indexes(name=\"facts_index\"))\n",
    "        if indexes and indexes[0].get('queryable'):\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    print(\"Done ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1b8fc-d119-475b-a5d1-80965b92ddb5",
   "metadata": {},
   "source": [
    "## Configure the Vector Store in LlamaIndex\n",
    "\n",
    "* The `MongoDBAtlasVectorSearch` represents a collection of documents, and a vector index.\n",
    "* The `VectorStoreIndex` is a combination of the store object, along with the embedding used to store data and query it.\n",
    "* A query engine is built from the Index, and contains the parameters used to query the index, and interpret results (ie. it also knows what LLM to use to interpret results\n",
    "* Finally, a `QueryEngineTool` is derived from the query engine and a description telling the LLM what the tool can be used to look up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MongoDB Vector Search\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    mongodb_client=mongodb_client,\n",
    "    db_name=mongodb_client.get_default_database().name,\n",
    "    collection_name=\"facts\",\n",
    "    vector_index_name=\"facts_index\",\n",
    ")\n",
    "\n",
    "# Represents a store and an embedding:\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "# Used to query the data:\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, llm=anthropic_llm)\n",
    "\n",
    "# A tool! This is generated from the query_engine,\n",
    "# combined with instructions to the LLM about what data can be looked up with it.\n",
    "facts_tool = QueryEngineTool(query_engine=query_engine,\n",
    "                             metadata=ToolMetadata(\n",
    "        name=\"facts\",\n",
    "        description=(\n",
    "            \"Provides facts to help you make decisions.\"\n",
    "            \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd198fc-0e22-42c2-ae8d-0125ac2c6c62",
   "metadata": {},
   "source": [
    "## Load in some facts from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1106a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some facts:\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# from llama_index.storage.docstore.mongodb import MongoDocumentStore\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"./facts\", required_exts=[\".txt\"])\n",
    "documents = reader.load_data()\n",
    "\n",
    "# create parser and parse document into nodes\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=MetadataMode.EMBED)\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "\n",
    "# build index\n",
    "facts_collection.delete_many({})\n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f2f0f-5d6c-44d1-827d-e6d087342c91",
   "metadata": {},
   "source": [
    "# Test The Assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a85cb-0d43-4d4d-b814-f0981945db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, add_tool, send_sms_tool, log_action_tool, facts_tool], llm=anthropic_llm, verbose=True)\n",
    "\n",
    "response = agent.chat(\"It is September 15th. Look up any relevant facts and act accordingly.\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
